# Guardrails

## What are guardrails?

Guardrails are protective mechanisms that increase the likelihood of your Large Language Model (LLM) application behaving appropriately and as intended. For example, you may have a chatbot explaining the different healthcare financing schemes to the public. In this case, you would want to ensure that the chatbot does not generate harmful nor inappropriate content such as medical advice.

At GovTech, we use the term guardrails to refer specifically to _separate components_ from the LLM itself that can filter or even adjust harmful or undesired content before it is generated by the LLM or returned to the user. The diagram below best illustrates how guardrails work in an AI system. As shown, prompts are first passed through an input guardrail before being sent to the LLM. Correspondingly, the output from the LLM is then passed through an output guardrail before being returned to the user. While it is true that LLMs do have some built-in safety mechanisms baked in through their training process or that API providers may have their own guardrails when they process the prompts we provide, we find it useful to have guardrails that build upon this for both performance and flexibility reasons that we will articulate below.

![Guardrails](guardrails/images/guardrail_simplified.png)
_Figure 1: High-level overview of guardrails in an AI system_

Guardrails can be as simple as a keyword check. For example, does the input or output contain any keywords defined in a blacklist of terms (e.g., common profanities or disallowed topics)? However, such an approach may not always be effective and does not leverage upon the predictive capabilities of machine learning models. Thus, more modern implementations would likely incorporate some machine learning too, in order to capture the semantics of the statement. Additionally, these guardrails are also multi-layered, ensuring overall robustness.

!!! note

    We can define guardrails as a classification problem.

## Principles for Effective Guardrails

In our last year of work on guardrails, we have identified three principles that are crucial for effective guardrails.

### 1. Performant, Localised, and Fast

Firstly, guardrails should be **performant, localised, and fast**. On performance, there is already a trade-off between false positives and false negatives, or what we call the "precision vs recall" trade-off in machine learning parlance. Having a guardrail that is strict may allow us to correctly identify more harmful content, but potentially at the cost of wrongly flagging more harmless content. Correspondingly, a guardrail that is lenient may wrongly flag more harmful content, but at the cost of fewer harmless content being wrongly flagged.

Additionally, performance requires some degree of localisation. One example of localisation is language and culture. For example, a guardrail that is trained based on the internet may not accuracy capture the naunces of terms used in Singapore. That said, localisation can also refer to localisation to the business context. For example, a specific acronym or terminology that is common in one industry may not be used in another. Hence, when testing guardrails, we should also test them in the context of the business.

Aside from this precision and recall trade-off, there is also the trade-off between **accuracy and latency**. Guardrails should operate efficiently without introducing significant latency, ensuring real-time feedback and a smooth user experience. Being performant also means that the guardrails can accurately identify harmful content.

### 2. Model-Agnostic Design

As defined above, we treat Guardrails as _separate components_ from the LLM itself. By doing so, we can separate develop and test the different components independently. This also offers us the flexibility to swap out different guardrails and LLMs, depending on the scenario. 

Having the guardrail as a separate component also allows us to **determine the minimum safety performance of our system**. Typical implementations of guardrails tend to be more deterministic compared to the variability of LLM outputs. Hence, if a guardrail has 95% accuracy in detecting not suitable for work (NSFW) language, then the entire system’s safety level is also at least 95%, leaving the model to deal with the remainder. Moreover, the underlying LLM or API provider may also change their own implementations. Hence, having separate guardrails provides us with more certainty on the holistic performance of the system.

### 3. Actionable and configurable

Instead of providing a simple binary response, guardrails should offer confidence or severity scores. This detailed feedback enables developers to implement differentiated actions such as logging, warning, modifying, or blocking content based on the specific use case. Balancing precision (accurately identifying harmful content) and recall (capturing all instances of harmful content) is crucial; this balance should be adjusted according to the application's context—for example, prioritizing recall in healthcare settings where safety is paramount, or emphasizing precision in customer service to enhance user experience.

## Guardrails Establish a Minimum Safety Standard

One advantage of separating the guardrail 


