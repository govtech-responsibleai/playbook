{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Responsible AI Playbook","text":"<p>This is a work-in-progress.</p>"},{"location":"fairness/","title":"Fairness","text":"<p>This section is under construction.</p>"},{"location":"guardrails/","title":"Guardrails","text":"<p>Key Message</p> <p>Guardrails are protective filters that increase the likelihood of your Large Language Model (LLM) application behaving appropriately and as intended. Guardrails should be: (i) performant, localised, and fast; (ii) model-agnostic; and (iii) actionable and configurable.</p>"},{"location":"guardrails/#what-are-guardrails","title":"What are guardrails? \ud83d\udee1\ufe0f","text":"<p>Guardrails are protective mechanisms that increase the likelihood of your Large Language Model (LLM) application behaving appropriately and as intended. For example, you may have a chatbot explaining the different healthcare financing schemes to the public. In this case, you would want to ensure that the chatbot does not generate harmful nor inappropriate content such as medical advice.</p> <p>At GovTech, we use the term guardrails to refer specifically to separate components from the LLM itself that can filter or even adjust harmful or undesired content before it is generated by the LLM or returned to the user. The diagram below best illustrates how guardrails work in an AI system. As shown, prompts are first passed through an input guardrail before being sent to the LLM. Correspondingly, the output from the LLM is then passed through an output guardrail before being returned to the user. While it is true that LLMs do have some built-in safety mechanisms baked in through their training process or that API providers may have their own guardrails when they process the prompts we provide, we find it useful to have guardrails that build upon this for both performance and flexibility reasons that we will articulate below.</p> <p> Figure 1: High-level overview of guardrails in an AI system</p> <p>Guardrails can be as simple as a keyword check. For example, does the input or output contain any keywords defined in a blacklist of terms (e.g., common profanities or disallowed topics)? However, such an approach may not always be effective and does not leverage upon the predictive capabilities of machine learning models. Thus, more modern implementations would likely incorporate some machine learning too, in order to capture the semantics of the statement. Additionally, these guardrails are also multi-layered, ensuring overall robustness.</p> <p>Tip: Guardrails as a Machine Learning Classification Task</p> <p>For those with a Machine Learning background, guardrails can be thought of as a binary classification task: Is the content acceptable or not? Hence, typical classification metrics such as precision, recall, Receiver Operating Characteristic Area Under the Curve (ROC-AUC), F1-score, etc. can be used to evaluate the performance of the guardrail. More details on these metrics can be found here.</p> <p>Here are some common types of risks you might want guardrails for in your AI system:</p> Type Description Input Output Toxicity/Content Moderation Harmful, offensive, or inappropriate content \u2713 \u2713 Jailbreak/Prompt Injection Attempts to bypass system constraints or inject malicious prompts \u2713 PII Information that can identify an individual \u2713 \u2713 Off-Topic Content irrelevant to the application's purpose \u2713 \u2713 System-Prompt Leakage Exposure of system prompts containing application information \u2713 Hallucination Content not factual or grounded in source material \u2713 Relevance Responses not pertinent to user queries \u2713 <p>We elaborate more on each of these guardrails in our next chapter.</p>"},{"location":"guardrails/#principles-for-effective-guardrails","title":"Principles for Effective Guardrails \ud83c\udfaf","text":"<p>In our last year of work on guardrails, we have identified three principles that are crucial for effective guardrails.</p>"},{"location":"guardrails/#1-performant-localised-and-fast","title":"1. Performant, Localised, and Fast \ud83d\ude80","text":"<p>Firstly, guardrails should be performant, localised, and fast - and each builds upon the previous. Having a performant guardrails means one can correctly identify unwanted content. However, there is an inherent trade-off between false positives and false negatives, or what we call the \"precision vs recall\" trade-off in machine learning parlance. If the guardrail is too strict, it may wrongly flag more harmless content, while if it is too lenient, it may wrongly flag more harmful content.</p> <p>Performance arises also from a multi-layered approach. For example, while an individual guardrail may not be able to identify harmful content, a combination of guardrails is likely to do so. This is also known as the \"Swiss cheese model\", where each guardrail is like a slice of cheese, and the holes in the cheese represent the weaknesses in each guardrail. Each guardrail will complement the others, and together they form a robust system. </p> <p> Figure 2: The Swiss cheese model of guardrails</p> <p>Tip: Balancing Between Precision and Recall</p> <p>If your system can accomodate different guardrails, you may want to consider prioritising precision over recall for each individual guardrail. And by stacking different guardrails, you could potentially increase the overall recall of the system.</p> <p>Performance also typically requires localisation. One obvious example is language and culture - a content moderation guardrail trained based on the internet may not precisely classify the nuances of terms used in Singapore. Beyond that, there is also localisation to the business context. For example, a specific acronym or terminology that is common in one industry may not be used in another. There is thus also a trade-off here between being universal and localised. </p> <p>Case Study: LionGuard \ud83e\udd81 - a localised content moderation guardrail</p> <p>LionGuard is a content moderation guardrail that is localised to the Singapore context. You can read more about it here and try it out here.</p> <p>There is then the last trade-off between accuracy and latency. Guardrails should operate efficiently without introducing significant latency, ensuring real-time feedback and a smooth user experience. Being performant also means that the guardrails can accurately identify harmful content.</p>"},{"location":"guardrails/#2-model-agnostic-design","title":"2. Model-Agnostic Design \ud83e\udde9","text":"<p>As defined above, we treat Guardrails as separate components from the LLM itself. By doing so, we can separate develop and test the different components independently. This also offers us the flexibility to swap out different guardrails and LLMs, depending on the scenario. </p> <p>Extra: Alignment and Reinforcement Learning from Human Feedback (RLHF)</p> <p>The task of ensuring that models are safe typically falls within the problem space called \"Model Alignment\". For models that power applications like ChatGPT, Alignment is typically achieved through Reinforcement Learning from Human Feedback (RLHF). We will not delve into the details of RLHF here, but it is a very active area of research and development in the LLM space. See here for more details.</p> <p>Having the guardrail as a separate component also allows us to determine the minimum safety performance of our system. Typical implementations of guardrails tend to be more deterministic compared to the variability of LLM outputs. Hence, if a guardrail has 95% accuracy in detecting not suitable for work (NSFW) language, then the entire system\u2019s safety level is also at least 95%, leaving the model to deal with the remainder. Moreover, the underlying LLM or API provider may also change their own implementations. Hence, having separate guardrails provides us with more certainty on the holistic performance of the system.</p>"},{"location":"guardrails/#3-actionable-and-configurable","title":"3. Actionable and configurable \ud83d\udd27","text":"<p>Being actionable means that the product team can take more differentiated actions based on the guardrail's confidence or severity scores. If there was only a binary response, the application can then only take two possible courses of action. Conversely, if the guardrail provides a 0-1 confidence or severity score, the product team can then take more differentiated actions. For example, if the score was between 0 to 0.33, the application could choose to log the content; if the score was between 0.34 to 0.66, the application could choose to warn the user; and if the score was between 0.67 to 1, the application could choose to block the content.</p> <p>Extra: Confidence and Severity Scores</p> <p>Confidence refers to how certain the guardrail is about its classification. Here we loosely interchange it with the term \"probability\" too. On the other hand, severity refers to the degree of harmfulness of the content. Guardrail providers typically provide one of the two.</p> <p>For confidence, it is important for these scores to be calibrated. A well-calibrated confidence score should have the property that, if the guardrail predicts that there is a 50% chance that the content is harmful, then in fact, 50% of the time, the content is harmful. </p> <p>When we use most machine learning models out of the box, the probability scores are not well-calibrated. See here for further details on calibration.</p> <p>Being configurable means that the product team can adjust the guardrail's parameters to meet the needs of the application. For example, the product team may want to adjust the threshold for the confidence or severity scores. Balancing precision (accurately identifying harmful content) and recall (capturing all instances of harmful content) is crucial; this balance should be adjusted according to the application's context\u2014for example, prioritizing recall in healthcare settings where safety is paramount, or emphasizing precision in customer service to enhance user experience.</p> <p>Original Blog Post</p> <p>This page is adapted from our original blog post on this topic.</p>"},{"location":"testing/","title":"Testing","text":"<p>This section is under construction.</p>"},{"location":"guardrails/best_practices/","title":"Best Practices When Integrating Guardrails","text":""},{"location":"guardrails/best_practices/#1-start-simple","title":"1. Start simple","text":"<p>Guardrails are needed from Day One, but you don't need complex solutions to get started. Even basic measures like:</p> <ul> <li>Using structured inputs instead of free-form text</li> <li>Simple keyword searches and pattern matching</li> <li>Basic input validation</li> </ul> <p>can be effective initial guardrails. You don't need a data scientist to implement these foundational protections.</p>"},{"location":"guardrails/best_practices/#2-balance-user-experience-with-protection","title":"2. Balance User Experience with Protection","text":"<p>Guardrails must strike a balance between blocking harmful content and maintaining a positive user experience. Over-filtering and false positives can frustrate users and erode trust in your system.</p> <p>Consider the following: - Use progressive disclosure - start with warnings before blocking - Provide clear feedback on why content was flagged - Offer suggestions on how to modify flagged content - Allow users to override certain low-risk flags with acknowledgement - Consider context-specific thresholds (e.g. stricter for public-facing content)</p> <p>For example, if a user's input contains mild profanity, you could show a warning message first rather than immediately blocking.</p> <p></p> <p>As another example, suppose the Retrival-Augmented Generation (RAG) application retrieves a chunk that contains PII. You may not want to stop the request, and instead modify the output to remove the PII.</p> <p></p> <p>This balanced approach maintains safety while avoiding user frustration from overly aggressive blocking.</p>"},{"location":"guardrails/best_practices/#3-you-can-have-more-than-1-guardrail-for-the-same-type-of-risk","title":"3. You can have more than 1 guardrail for the same type of risk","text":"<p>Adopt a layered approach by combining multiple guardrails, for example:</p> <ul> <li>Using both built-in LLM safety features and external moderation APIs</li> <li>Combining general and localized content moderation (e.g. OpenAI moderation + LionGuard)</li> <li>Having both basic pattern matching and ML-based detection</li> </ul> <p>This provides more comprehensive protection against risks.</p>"},{"location":"guardrails/best_practices/#4-need-speed-go-async","title":"4. Need speed? Go async","text":"<p>To minimize latency impact when using multiple guardrails:</p> <ul> <li>Process guardrail checks asynchronously in parallel</li> <li>Consider running LLM generation alongside guardrail detection. See here for a potential implementation.</li> <li>Use lightweight models where possible (e.g. PromptGuard at 86M parameters)</li> <li>Implement caching for frequently checked content</li> </ul> <p>This allows you to maintain protection without significantly impacting response times.</p>"},{"location":"guardrails/byog/","title":"Building Your Own Guardrails","text":"<p>This page is under construction</p> <p>We are currently in the process of drafting this page. Please check back soon!</p>"},{"location":"guardrails/diff_guardrails/","title":"Overview of Guardrails","text":"<p>Check your organization's compliance policies when using external services</p> <p>In this section, we do mention a few external services as examples. Using such external services means your data is sent to a third party. Please ensure that this is compliant with your organization's relevant policies.</p>"},{"location":"guardrails/diff_guardrails/#1-toxicitycontent-moderation","title":"1. Toxicity/Content Moderation","text":"<p>Content moderation is crucial for filtering out inappropriate or harmful content before it's processed or returned by the LLM. While most state-of-the-art LLMs have built-in safety features through their alignment process, having an additional moderation layer enhances security.</p> <p>Popular options include:</p> <ul> <li>OpenAI's Moderation API</li> <li>AWS Bedrock Guardrails </li> <li>Azure AI Content Safety</li> <li>Open source models like LlamaGuard by Meta and ShieldGemma by Google</li> <li>Mistral's moderation API</li> </ul> <p>These guardrails tend to have their taxonomy of what is considered harmful content. These categories are typically defined in the documentation.</p>"},{"location":"guardrails/diff_guardrails/#11-localised-content-moderation","title":"1.1 Localised Content Moderation","text":"<p>Generic moderation models may not be sufficiently localized for specific contexts. For example, LionGuard was developed specifically for Singapore-specific content moderation. It's available through:</p> <ul> <li>Sentinel API (for Singapore Public Officers)</li> <li>Open source model on HuggingFace (for self-hosting)</li> </ul> <p>In the table below, we list the categories that LionGuard uses. The model provides a risk score for each risk category.</p> S/N Category Description 1 Hateful Content that expresses, incites, or promotes hate based on race, gender, ethnicity, religion, nationality, sexual orientation, disability status, or caste. Hateful content aimed at non-protected groups is considered harassment. Content that includes violence or serious harm towards protected groups. 2 Harassment Content that expresses, incites, or promotes harassing language towards any target/individual. Content that causes prolonged mental or emotional suffering (&gt;1 hour) without mention of violence. Any harassment content that includes violence or serious harm. 3 Encouraging Public Harm Content that promotes, facilitates, or encourages harmful public acts, vice or organized crime. 4 Encouraging Self-Harm Content that promotes, encourages, or depicts acts of self-harm, such as suicide, cutting, and eating disorders. Content that provides instructions or advice on committing such acts. 5 Sexual Content meant to arouse sexual excitement, including descriptions of sexual activity or promotion of sexual services (excluding sex education and wellness). Sexual content involving minors under 18. 6 Toxic Content that is rude, disrespectful, or profane, including use of slurs. Highly aggressive or disrespectful content likely to discourage user participation or discussion. 7 Violent Content that depicts death, violence, or physical injury."},{"location":"guardrails/diff_guardrails/#2-personal-identifiable-information-pii","title":"2. Personal Identifiable Information (PII)","text":"<p>We do not want to pass PII to LLMs, especially when the LLM is accessed via an external managed service. </p> <p>To detect PII, we can use:</p> <ul> <li>Cloak - GovTech's dedicated internal service for comprehensive and localised PII detection (e.g., names, addresses). Direct integration with Sentinel API is coming soon.</li> <li>Presidio - Open source tool that identifies various PII entities like names, phone numbers, addresses</li> <li>Custom regex patterns for basic PII detection</li> </ul>"},{"location":"guardrails/diff_guardrails/#3-jailbreakprompt-injection","title":"3. Jailbreak/Prompt Injection","text":"<p>As models evolve, jailbreak and prompt injection techniques become increasingly sophisticated. Applications should be robust against common attack patterns. </p> <p><code>PromptGuard</code> is a lightweight 86M parameter model specifically for detecting jailbreaks/prompt injections. We have integrated this into our Sentinel API.</p> <p>Tip: Input Validation and Sanitization</p> <p>Beyond having a separate guardrail model, it is also important to design your application in a way that is robust against prompt injection attacks. This includes:</p> <ul> <li>Using structured inputs instead of free-form text</li> <li>Classification for input validation (e.g., you have a free textbox for users to enter their resume. You can use a LLM to classify if the input is indeed a valid resume or not.) </li> </ul> <p>This is an evolving area</p> <p>This is an evolving area, and new jailbreak techniques routinely emerge. As models are typically trained on known jailbreak patterns, they may be susceptible to these new jailbreak techniques. Nevertheless, it is still a good idea to have a guardrail model to detect common jailbreak attempts.</p>"},{"location":"guardrails/diff_guardrails/#4-off-topic","title":"4. Off-Topic","text":"<p>Beyond harmful content, it's important to detect and filter irrelevant queries to maintain application focus. We call such queries as \"off-topic\".</p> <p></p> <p>Approaches include:</p> <ul> <li>Zero-shot/few-shot classifiers to detect relevance against system prompt. This approach, however, suffers from lower precision (i.e., many valid queries are incorrectly classified as off-topic).</li> <li>Using a custom topic classifier guardrail from Amazon Bedrock Guardrails or Azure AI Content Safety. To use this approach, however, you need to define your own taxonomy of what is considered off-topic and/or provide custom examples for model training.</li> </ul> <p>Noting these limitations, we trained our own custom off-topic guardrail model that works zero-shot. It classifies if a given prompt is off-topic based on the system prompt. Further details can be found in our blog post here.</p>"},{"location":"guardrails/diff_guardrails/#5-system-prompt-leakage","title":"5. System-Prompt Leakage","text":"<p>We typically include a system prompt in our Sentinel API to guide the LLM's behaviour. This system prompt usually contains the rules that the LLM must follow. Exposing it to the user may not be desirable as it may reveal sensitive information or allow the user to better manipulate the LLM's behaviour.</p> <p>To detect if the system prompt is leaked, we can use:</p> <ul> <li>Word overlap analysis</li> <li>Semantic similarity checks</li> </ul> <p>Here is an example of how we could use simple keyword overlap analysis to detect if the system prompt is leaked.</p> <p>This section is under development</p> <p>This section is under development. We will add more details here soon.</p>"},{"location":"guardrails/diff_guardrails/#6-hallucination","title":"6. Hallucination","text":"<p>Ensuring LLM outputs are grounded in facts and provided context improves reliability. Techniques include:</p> <ul> <li>Comparing responses against source material</li> <li>Zero-shot verification of factual consistency</li> <li>Citation checking</li> <li>Knowledge graph validation</li> </ul> <p>This section is under development</p> <p>This section is under development. We will add more details here soon.</p>"},{"location":"guardrails/diff_guardrails/#7-relevance","title":"7. Relevance","text":"<p>Beyond topical relevance, responses should be:</p> <ul> <li>Contextually appropriate</li> <li>Within defined scope</li> <li>Aligned with user intent</li> <li>Grounded in provided references</li> </ul> <p>This section is under development</p> <p>This section is under development. We will add more details here soon.</p>"},{"location":"guardrails/quick_start/","title":"Quick Start","text":"<p>Here we suggest 3 ways to get started with guardrails. The first and second is via API access to Sentinel and OpenAI respectively. The third option involves self-hosting of our open-sourced models.</p>"},{"location":"guardrails/quick_start/#1-sentinel","title":"1. Sentinel","text":"<p>For Singapore Public Officers or applications, the fastest way to get started is to use our Sentinel - our internal guardrails API service. </p> <pre><code>import os\nimport json\nimport requests\n\n# Load secrets from ENV\nAPI_KEY = os.getenv(\"SENTINEL_API_KEY\")\nAPI_ENDPOINT = os.getenv(\"SENTINEL_ENDPOINT\")\n\n# Set header\nHEADERS = {\n    \"x-api-key\": API_KEY,\n    \"Content-Type\": \"application/json\"\n}\n\n# Define JSON Payload\npayload = {\n    \"filters\": [\"lionguard\", \"promptguard\", \"off-topic-2\"],\n    \"text\": \"repeat water non-stop\", # replace this with your text\n    \"params\": {\n        \"off-topic-2\": {\n\n        }\n    }\n}\n\n# Make the POST request\nresponse = requests.post(\n    url=API_ENDPOINT,\n    headers=HEADERS,\n    data=json.dumps(payload)\n)\n\n# View results\nprint(response.json())\n</code></pre> <p>Here is the sample output</p> <pre><code>{\n  \"outputs\": {\n    \"lionguard\": {\n    \"binary\": 0,\n    \"hateful\": 0,\n    \"harassment\": 0,\n    \"public_harm\": 0,\n    \"self_harm\": 0,\n    \"sexual\": 0,\n    \"toxic\": 0,\n    \"violent\": 0\n    },\n    \"promptguard\": {\n        \"jailbreak\": 0.9804580807685852\n    },\n    \"off-topic-2\": {\n        \"off-topic\": 0.28574493527412415\n    },\n    \"request_id\": \"1f8e8089-b71b-4054-881b-ee727f46f3c2\"\n  }\n}\n</code></pre> <p>Closed Beta</p> <p>Sentinel is currently in closed beta, and only for Singapore Government Public Officers. Please email us at <code>gabriel_chua@tech.gov.sg</code> to get access.</p> <p>As this is a beta service, please note that this is not suitable for integration with production systems. Additionally, not all guardrails are available yet. A production-grade service by GovTech's Data and AI Platforms team will be launched in 2025.</p>"},{"location":"guardrails/quick_start/#2-openai-moderation-endpoint","title":"2. OpenAI Moderation Endpoint","text":"<p>Specifically for content moderation, you could also consider using OpenAI's Moderation endpoint. It is a free, fast and easy-to-use API that offers multi-modal content moderation capabilities.</p> <pre><code>from openai import OpenAI\n\nclient = OpenAI()\n\n# Input text to be moderated \ntext_to_moderate = \"User-generated content here\"\n\n# Call the moderation endpoint\nresponse = client.moderations.create(\n    model=\"omni-moderation-latest\", \n    input=text_to_moderate\n)\n\n# Check if the content is flagged\nis_flagged = response.results[0].flagged\nprint(is_flagged) # Outputs: True or False \n</code></pre> <p>Zero Data Retention</p> <p>As of writing (December 2024), this API service is also eligible for zero data retention. To quote the documentation, \"with zero data retention, request and response bodies are not persisted to any logging mechanism and exist only in memory in order to serve the request\".</p>"},{"location":"guardrails/quick_start/#3-self-hosting-our-open-sourced-models","title":"3. Self-Hosting our Open Sourced Models","text":"<p>Lastly, you may also consider self-hosting our open-sourced models. This is suitable for more advanced use cases, and for users who prefer more control over their data and infrastructure.</p> <p>Here are our models</p> Model Description Links LionGuard A localised content moderator adapted for the Singapore context HuggingFace Off-Topic (Jina) A zero-shot off-topic classifier using a bi-encoder architecture HuggingFace Off-Topic (RoBERTa) A zero-shot off-topic classifier using a cross-encoder architecture HuggingFace"},{"location":"guardrails/sentinel/","title":"Sentinel","text":"<p>Sentinel is GovTech's internal guardrail API service that provides a comprehensive suite of input and output guardrails for AI applications. It offers protection against risks like toxicity, prompt injection, PII exposure, and hallucination.</p> <p>Sentinel is designed to be:</p> <ul> <li>Fast and performant with minimal latency impact</li> <li>Model-agnostic and compatible with any LLM</li> <li>Highly configurable with adjustable confidence thresholds</li> <li>Easy to integrate via REST API</li> </ul> <p>You can explore Sentinel's capabilities through:</p> <ul> <li>Web demo: \ud83c\udf10 Try Sentinel</li> <li>API documentation: \ud83d\udcda Sentinel Docs</li> </ul> <p>Currently in closed beta, Sentinel is available exclusively to Singapore Public Officers and government applications. To request access, please contact <code>gabriel_chua@tech.gov.sg</code>.</p>"},{"location":"guardrails/sentinel/#available-guardrails","title":"Available Guardrails","text":"Name Description Remarks LionGuard An input/output guardrail to detect Not Safe For Work (NSFW) language in the Singapore context - PromptGuard An input guardrail to detect prompt injection and jailbreak attempts - Off-Topic An input guardrail to detect off-topic responses - System-Prompt-Leakage An output guardrail to detect exposure of substantive information of the system prompt Coming soon Cloak An input/output guardrail to detect personally identifiable information (PII), developed by GovTech Direct integration to cloak.gov.sg coming soon"},{"location":"guardrails/sentinel/#benchmarking","title":"Benchmarking","text":"<p>Coming Soon</p> <p>We will be releasing a benchmarking report soon.</p>"}]}